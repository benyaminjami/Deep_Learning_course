{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate):\n",
    "        # Build the network to predict the correct action\n",
    "        tf.reset_default_graph()\n",
    "        input_dimension = 4\n",
    "        hidden_dimension = 32\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[1, input_dimension], name='X')\n",
    "        hidden_layer = tf.layers.dense(self.input, hidden_dimension, kernel_initializer=tf.initializers.random_normal())\n",
    "        logits = tf.layers.dense(hidden_layer, 2, kernel_initializer=tf.initializers.random_normal())\n",
    "\n",
    "        # Sample an action according to network's output\n",
    "        # use tf.multinomial and sample one action from network's output\n",
    "        self.action = tf.random.categorical(logits, 1)\n",
    "\n",
    "        # Optimization according to policy gradient algorithm\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(tf.one_hot(self.action, 2), logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate)  # use one of tensorflow optimizers\n",
    "        grads_vars = self.optimizer.compute_gradients(cross_entropy)  # gradient of current action w.r.t. network's variables\n",
    "        self.gradients = [grad for grad, var in grads_vars]\n",
    "\n",
    "        # get rewards from the environment and evaluate rewarded gradients\n",
    "        #  and feed it to agent and then call train operation\n",
    "        self.rewarded_grads_placeholders_list = []\n",
    "        rewarded_grads_and_vars = []\n",
    "        for grad, var in grads_vars:\n",
    "            rewarded_grad_placeholder = tf.placeholder(dtype=tf.float32, shape=grad.shape)\n",
    "            self.rewarded_grads_placeholders_list.append(rewarded_grad_placeholder)\n",
    "            rewarded_grads_and_vars.append((rewarded_grad_placeholder, var))\n",
    "\n",
    "        self.train_operation = self.optimizer.apply_gradients(rewarded_grads_and_vars)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        config = tf.ConfigProto(\n",
    "            device_count={'GPU': 0},\n",
    "            gpu_options=tf.GPUOptions(allow_growth=True)\n",
    "        )\n",
    "\n",
    "        self.ses = tf.Session(config=config)\n",
    "        self.ses.run(tf.global_variables_initializer())\n",
    "\n",
    "    def get_action_and_gradients(self, obs):\n",
    "        # compute network's action and gradients given the observations\n",
    "        return self.ses.run([self.action, self.gradients], feed_dict={self.input: obs})\n",
    "\n",
    "    def train(self, rewarded_gradients):\n",
    "#         feed_dict = {(self.rewarded_grads_placeholders_list[i], rewarded_gradients[i]) for i in range(len(rewarded_gradients))}\n",
    "        # feed gradients into the placeholder and call train operation\n",
    "        feed_dict = {}\n",
    "        for i in range(len(rewarded_gradients)):\n",
    "            feed_dict[self.rewarded_grads_placeholders_list[i]] = rewarded_gradients[i]\n",
    "        self.ses.run([self.train_operation], feed_dict=feed_dict)\n",
    "\n",
    "    def save(self):\n",
    "        self.saver.save(self.ses, \"SavedModel/\")\n",
    "\n",
    "    def load(self):\n",
    "        self.saver.restore(self.ses, \"SavedModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, , Average reward = 45.65625\n",
      "Epoch = 1, , Average reward = 48.1328125\n",
      "Epoch = 2, , Average reward = 48.9375\n",
      "Epoch = 3, , Average reward = 48.6640625\n",
      "Epoch = 4, , Average reward = 51.5390625\n",
      "Epoch = 5, , Average reward = 53.4140625\n",
      "Epoch = 6, , Average reward = 52.375\n",
      "Epoch = 7, , Average reward = 57.5390625\n",
      "Epoch = 8, , Average reward = 56.03125\n",
      "Epoch = 9, , Average reward = 59.2421875\n",
      "Epoch = 10, , Average reward = 62.4296875\n",
      "Epoch = 11, , Average reward = 64.546875\n",
      "Epoch = 12, , Average reward = 64.546875\n",
      "Epoch = 13, , Average reward = 66.7109375\n",
      "Epoch = 14, , Average reward = 68.578125\n",
      "Epoch = 15, , Average reward = 77.7578125\n",
      "Epoch = 16, , Average reward = 75.7421875\n",
      "Epoch = 17, , Average reward = 85.578125\n",
      "Epoch = 18, , Average reward = 88.9375\n",
      "Epoch = 19, , Average reward = 100.2109375\n",
      "Epoch = 20, , Average reward = 109.4609375\n",
      "Epoch = 21, , Average reward = 124.8046875\n",
      "Epoch = 22, , Average reward = 145.921875\n",
      "Epoch = 23, , Average reward = 192.640625\n",
      "Epoch = 24, , Average reward = 218.3125\n",
      "Epoch = 25, , Average reward = 231.171875\n",
      "Epoch = 26, , Average reward = 252.953125\n",
      "Epoch = 27, , Average reward = 299.8671875\n",
      "Epoch = 28, , Average reward = 323.9765625\n",
      "Epoch = 29, , Average reward = 395.90625\n",
      "Epoch = 30, , Average reward = 374.484375\n",
      "Epoch = 31, , Average reward = 296.25\n",
      "Epoch = 32, , Average reward = 292.375\n",
      "Epoch = 33, , Average reward = 264.15625\n",
      "Epoch = 34, , Average reward = 265.328125\n",
      "Epoch = 35, , Average reward = 287.0078125\n",
      "Epoch = 36, , Average reward = 326.578125\n",
      "Epoch = 37, , Average reward = 419.515625\n",
      "Epoch = 38, , Average reward = 554.4375\n",
      "Epoch = 39, , Average reward = 990.9609375\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "max_steps_per_game = 1000\n",
    "games_per_epoch = 128\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.01\n",
    "\n",
    "def discount_function(factor, n):\n",
    "    return (1 - factor**n)/(1 - factor)\n",
    "\n",
    "agent = Agent(learning_rate)\n",
    "game = gym.make(\"CartPole-v0\").env\n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = []\n",
    "    epoch_gradients = []\n",
    "    epoch_average_reward = 0\n",
    "    for episode in range(games_per_epoch):\n",
    "        obs = game.reset()\n",
    "        step = 0\n",
    "        single_episode_rewards = []\n",
    "        single_episode_gradients = []\n",
    "        game_over = False\n",
    "        while not game_over and step < max_steps_per_game:\n",
    "            step += 1\n",
    "#             image = game.render(mode='rgb_array') # Call this to render game and show visual\n",
    "            action, gradients = agent.get_action_and_gradients(obs.reshape([1,4]))\n",
    "            obs, reward, game_over, info = game.step(action[0,0])\n",
    "            single_episode_rewards.append(reward)\n",
    "            single_episode_gradients.append(gradients)\n",
    "\n",
    "        epoch_rewards.append(single_episode_rewards)\n",
    "        epoch_gradients.append(single_episode_gradients)\n",
    "        epoch_average_reward += sum(single_episode_rewards)\n",
    "\n",
    "    epoch_average_reward /= games_per_epoch\n",
    "    print(\"Epoch = {}, , Average reward = {}\".format(epoch, epoch_average_reward))\n",
    "    \n",
    "    normalized_rewards = [[(discount_function(discount_factor, len(epoch_rewards[j])-i)-epoch_average_reward) for i in range(len(epoch_rewards[j]))] for j in range(len(epoch_rewards))]\n",
    "    mean_rewarded_gradients = [np.zeros(agent.gradients[i].shape) for i in range(4)]\n",
    "    for i in range(games_per_epoch):\n",
    "        for j in range(len(epoch_gradients[i])):\n",
    "            for k in range(4):\n",
    "                mean_rewarded_gradients[k] += (normalized_rewards[i][j] * epoch_gradients[i][j][k])/games_per_epoch\n",
    "#     print(mean_rewarded_gradients[1])\n",
    "#     break\n",
    "    agent.train(mean_rewarded_gradients)\n",
    "    if epoch_average_reward > 900:\n",
    "        break\n",
    "agent.save()\n",
    "game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from SavedModel/\n",
      "Average Score =  136798.5\n"
     ]
    }
   ],
   "source": [
    "# Run this part after training the network\n",
    "game = gym.make(\"CartPole-v0\").env\n",
    "agent.load()\n",
    "score = 0\n",
    "for i in range(10):\n",
    "    obs = game.reset()\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        score += 1\n",
    "        image = game.render(mode='rgb_array')  # Call this to render game and show visual\n",
    "        action, _ = agent.get_action_and_gradients(obs.reshape(-1, 4))\n",
    "        obs, reward, game_over, info = game.step(action[0,0])\n",
    "    # print(score)\n",
    "\n",
    "print(\"Average Score = \", score / 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
