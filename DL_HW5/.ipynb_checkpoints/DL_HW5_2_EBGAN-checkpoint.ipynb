{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11Fphw9kRjq8"
   },
   "source": [
    "## Homework 5 - 2:  EBGAN\n",
    "\n",
    "The goal is to train a GAN with an auto-encoder as its discriminator.\n",
    "For further information read the [paper of EBGAN](https://arxiv.org/abs/1609.03126).\n",
    "\n",
    "Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mu_pd9TVFb9v"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flZ1MOT8F8kE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rg-CYaG9GU03"
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "original_train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "original_test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZ06E4yEHbGo"
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXUYiAuzGWuh"
   },
   "outputs": [],
   "source": [
    "# Define Train loader\n",
    "train_tensors = original_train_dataset.data.float() / 255\n",
    "test_tensors = original_test_dataset.data.float() / 255\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensors, original_train_dataset.targets)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensors, original_test_dataset.targets)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "os1o4AE-FvGg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUx_-WpBFxRp"
   },
   "outputs": [],
   "source": [
    "def show(image_batch, rows=1):\n",
    "    # Set Plot dimensions\n",
    "    cols = np.ceil(image_batch.shape[0] / rows)\n",
    "    plt.rcParams['figure.figsize'] = (0.0 + cols, 0.0 + rows) # set default size of plots\n",
    "    \n",
    "    for i in range(image_batch.shape[0]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image_batch[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWlDL4yUE8f4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-P5Au4tHZAT"
   },
   "outputs": [],
   "source": [
    "class AutoEncoderMSE(nn.Module):\n",
    "    def __init__(self, input_dim, encoder_dims, decoder_dims, dropout_rate=0.5):\n",
    "        super(AutoEncoderMSE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        ########################## TODO ##########################\n",
    "        # self.input_dropout must be a dropout module with p=dropout_rate\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "        \n",
    "        \n",
    "        # Encoder part\n",
    "        encoder_layers = []\n",
    "        ########################## TODO ##########################\n",
    "        # Define encoder layers and add them to `encoder_layers`\n",
    "        # Use nn.LeakyReLU(0.2) for activation functions\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        last_encoder_dim = ([input_dim] + encoder_dims)[-1]\n",
    "\n",
    "        # Decoder part\n",
    "        decoder_layers = []\n",
    "        ########################## TODO ##########################\n",
    "        # Define decoder layers and add them to `decoder_layers`\n",
    "        # Use nn.LeakyReLU(0.2) for activation functions\n",
    "        # Last layer does not need any activation function\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        self.MSE = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, x):\n",
    "        ########################## TODO ##########################\n",
    "        # Complete the flow\n",
    "        # x > dropout > encoder > h > decoder > output > mse\n",
    "        # Note that mse.shape = (batch_size, )\n",
    "        # DO NOT FORGET TO ADD DROPOUT LAYER\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "\n",
    "      \n",
    "discriminator = AutoEncoderMSE(784, [256, 128, 64], [128, 256], dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liKLsKy2H_9a"
   },
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.Dropout(),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(512, 784),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPNWbLh5JW1L"
   },
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "  discriminator.cuda()\n",
    "  generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kv2HXF9gIQ-Q"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE_D = 0.0002\n",
    "LEARNING_RATE_G = 0.0002\n",
    "\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D)\n",
    "opt_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7L-wvColI2fW"
   },
   "outputs": [],
   "source": [
    "N_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6q_InsGI-ad"
   },
   "outputs": [],
   "source": [
    "m = 16\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.flatten(start_dim=1)\n",
    "\n",
    "        real_img = img\n",
    "        if CUDA:\n",
    "            real_img = real_img.cuda()\n",
    "\n",
    "        z = torch.randn(img.shape[0], 128)\n",
    "        if CUDA:\n",
    "            z = z.cuda()\n",
    "        fake_img = generator(z)\n",
    "\n",
    "        # Discriminator Part\n",
    "        opt_D.zero_grad()\n",
    "        ########################## TODO ##########################\n",
    "        # Define loss for discriminator\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "        loss_d.backward()\n",
    "        opt_D.step()\n",
    "        \n",
    "        # Generator Part\n",
    "        opt_G.zero_grad()\n",
    "        ########################## TODO ##########################\n",
    "        # Define loss for generator\n",
    "        pass\n",
    "        ######################## END TODO ########################\n",
    "        loss_g.backward()\n",
    "        opt_G.step()\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"epoch: {} \\t last batch loss D: {} \\t last batch loss G: {}\".format(epoch, loss_d.item(), loss_g.item()))\n",
    "    imgs_to_show = fake_img[:30].view(-1, 28, 28).detach().cpu().numpy()\n",
    "    show(imgs_to_show, rows=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFXCjH1LLzNB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_HW5_2_EBGAN_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
